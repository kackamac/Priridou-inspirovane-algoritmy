{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basics Functions a Recurrent Neural Network \n",
    "\n",
    "Minule jsme si ukazovali a zkoušeli si naprogramovat jednoduché neuronové sítě. Dneska už se podíváme na složitější a zajímavější struktury -- Radial Basics Functions a Rekurentní Neuronové sítě.\n",
    "\n",
    "## Radial Basics Functions (RBF)\n",
    "\n",
    "Základní myšlenkou RBF je, že prostor příznaků je rozdělený neurony jakožto lokálními jednotkami. Každý neuron generuje signál odpovídající vstupnímu vektoru a jeho síla závisí právě na vzdálenosti neuronu od vstupního vektoru.\n",
    "\n",
    "RBF je tedy neuronová síť s právě třemi vrstvami. Vstupní vrstva odpovídá vstupům do sítě, na skryté vrstvě se ale místo klasického skalárního součinu vstupů a vah počítají vzdálenosti vstupů od středů neuronů. Jakožto aktivační funkce se používají právě radial basics funkce. Často používanou funkcí je například Gaussova. Výstupní vrstva je normální vrstva neuronů, která počítá výstup sítě jako lineární kombinaci výstupů skryté vrstvy a příslušných vah.\n",
    "\n",
    "Trénování sítě má dvě fáze. Nejprve se pomocí algoritmu kmeans najdou středy (neboli váhy mezi vstupní a skrytou vrstvou). Poté se natrénují váhy na výstupu, na což nám stačí obyčejná lineární regrese, protože se jedná pouze o lineární kombinaci vektorů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujeme si nyní vlastní RBF síť. K tomu nám stačí naimplementovat třídu ```RBFNetwork```, která bude mít tři metody. Metoda ```init``` nainicializuje zadané parametry sítě,  metoda ```fit``` bude sloužit k nalezení středů a trénování vah a metoda ```predict``` pak bude predikovat pro dané vstupy jejich nejpravděpodobnější výstupy. Pomůžou nám ještě dvě pomocné funkce ```_activation_fcn``` a ```_calculate_activation```, kdy první z nich nám vrátí hodnotu aktivační (neboli Gaussovské) funkce pro dané středy a vstupní data a druhá nám pak s pomocí první spočítá hodnoty neuronů na skryté vrstvě. Ty se pak jen lineárně zkombinují s váhami a vrátí se jako výstup sítě."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork():\n",
    "    def __init__(self, input_dim, num_centers, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_centers = num_centers\n",
    "        self.centers = [np.random.uniform(-1, 1, input_dim) for i in range(num_centers)]\n",
    "        self.beta = 1\n",
    "        self.weights = None\n",
    "        \n",
    "    def _calculate_activation(self, data):\n",
    "        hidden_layer_values = np.zeros((data.shape[0], self.num_centers), float)\n",
    "        for c_idx, c in enumerate(self.centers):\n",
    "            for x_idx, x in enumerate(data):\n",
    "                hidden_layer_values[x_idx,c_idx] = self._activation_fcn(c, x)\n",
    "        return hidden_layer_values\n",
    "    \n",
    "    def _activation_fcn(self, centers, data):\n",
    "        return np.exp(-self.beta * np.linalg.norm(centers-data)**2)\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        \n",
    "        # zde chybi natrenovat centroidy :P\n",
    "        \n",
    "        # spocitame aktivaci mezi vstupem a skrytou vrstvou\n",
    "        hidden_layer_values = self._calculate_activation(data)\n",
    "         \n",
    "        # porovname skutecne a predikovane vystupy a aktualizujem vahy pomoci pseudoinverzni matice, \n",
    "        # coz je vlastne vzorecek pro LR pro train vah\n",
    "        self.weights = np.dot(np.linalg.pinv(hidden_layer_values), labels)\n",
    "          \n",
    "    def predict(self, data):\n",
    "        hidden_layer_values = self._calculate_activation(data)\n",
    "        labels = np.dot(hidden_layer_values, self.weights)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si výše napsanou RBF síť pustit na našem oblíbeném datasetu Iris. Načteme si data a labely, do které třídy data patří. Protože budeme dělat klasifikaci, je vhodné si labely převést na one-hot-encoding. Následně data rozdělíme na trénovací a testovací množinu, abychom mohli zvlášť data trénovat a zvlášť data testovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "onehotencoder = OneHotEncoder() \n",
    "y_train_onehot = onehotencoder.fit_transform(y_train.reshape(-1, 1)).toarray() \n",
    "y_test_onehot = onehotencoder.fit_transform(y_test.reshape(-1, 1)).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní už jen nainicializujeme síť a necháme ji, aby se trénovala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34210526315789475\n"
     ]
    }
   ],
   "source": [
    "rbf = RBFNetwork(4, 5, 3)\n",
    "rbf.fit(x_train, y_train_onehot)\n",
    "predicted = rbf.predict(x_test)\n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print('Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že accuracy je v každém běhu dosti různá, což je způsobeno tím, že centroidy a betu zde nastavujeme náhodně, přestože jsme si říkali, že pozice centroidů se dá trénovat pomocí algoritmu k-means.\n",
    "\n",
    "### Úkol na cvičení\n",
    "\n",
    "Algoritmus kmeans je jednou z metod učení bez učitele. V závislosti na vzájemné vzdálenosti dat je rozdělí do předem určeného počtu skupin neboli klastrů. Každý klastr je určen svým středem a body ze vstupních dat, které mejí tento střed jako svůj nejbližší. \n",
    "\n",
    "Algoritmus funguje následovně. Na začátku si zvolíme počet klastrů $k$ a vybereme náhodné body jako středy klastrů. Potom se pro každý bod z dat najde nejbližší střed. Následně se podle bodů, které byly danému středu přiřazeny přepočítají nové hodnoty středů. Algoritmus se iterativně opakuje, dokud se pozice středů mění. \n",
    "\n",
    "Zkuste si naimplementovat algoritmus k-means pro inicializaci středů vstupních neuronů a zlepšit tím výstup sítě výše.\n",
    "\n",
    "## Rekurentní neuronové sítě\n",
    "\n",
    "Rekurentní neuronová síť je síť, která navíc ke svému vstupu ještě bere jako další vstup svůj výstup z předchozího kroku. Proto výstupy z předchozí vrstvy mohou ovlivňovat vrstvu následující, což se může hodit například u generování časových řad nebo textu. \n",
    "\n",
    "Nejprve se podíváme, jak přesně by vypadala implementace jednoduché RNN, kdybychom si ji psali celou sami. \n",
    "\n",
    "Vytvoříme si nejprve jednoduché věty jako trénovací a testovací dataset a budeme chtít, aby nám naše sít uměla predikovat, zda je daná věta pozitivní nebo negativní. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve je potřeba udělat nějaký preprocessing vět. Tím se myslí převést text do číselné reprezentace. To uděláme tak, že najdeme všechna unikátní slova, očíslujeme je, a každé slovo pak nahradíme jeho číslem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Můžeme se pro zajímavost podívat na náš slovník a na to, které slovo je na určitém indexu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'at', 'and', 'now', 'earlier', 'was', 'bad', 'sad', 'i', 'very', 'is', 'not', 'this', 'right', 'or', 'happy', 'am', 'good']\n",
      "Unique words: 18\n",
      "happy has index: 15\n",
      "On index 0 is a word: sad\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print('Unique words: ' + str(vocab_size))\n",
    "print(f'happy has index: {word_to_idx[\"happy\"]}') \n",
    "print(f'On index 0 is a word: {idx_to_word[7]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Následně si napíšeme funkci na vytvoření one-hot-encoding každého slova, které potom bude mít shape ```(vocab_size, 1)``` a také funkci na preprocessing dat, kterou použijeme na data před trénováním a testováním. Konkrétně převedeme všechna slova pomocí one hot encodingu na vektory a výsledné labely na integery 0 nebo 1 podle toho, zda se jedná o negativní, či pozitivní větu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_input(text):\n",
    "    inputs = []\n",
    "    for w in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs\n",
    "\n",
    "def parse_data(data):\n",
    "    data_parsed = list(data.items())\n",
    "    random.shuffle(data_parsed)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for x, y in data_parsed:\n",
    "        features.append(create_one_hot_input(x))\n",
    "        labels.append(int(y))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní si napíšeme samotnou RNN. Informace z předchozího kroku se ukládá v RNN buňce do speciální proměnné -- tzv. skrytého stavu, který nám pak bude ovlivňovat další výstupy a bude se s každým novým vstupem v každém kroku aktualizovat. Aktuální skrytý stav se počítá podle předchozího skrytého stavu a aktuálního vstupu. Výstup je spočítán pomocí aktuálního skrytého stavu. Pro každý krok se používají stejné 3 váhové matice: \n",
    "- pro spoje z aktuálních vstupů do aktuálních skrytých, \n",
    "- pro spoje z předchozích skrytých do aktuálních skrytých\n",
    "- pro spoje z aktuálních skrytých do vystupů. \n",
    "\n",
    "Zároveň potřebujeme jeden bias pro spočtení skrytého stavu a další pro spočtení výstupu. Pomocí aktivační funkce $tanh$ a dosazení hodnot do rovnice umíme vypočítat výstup a update skrytých stavů. Pomocná funkce ```softmax``` slouží jako aktivace pro namapování hodnot do intervalu [0,1], aby nám funkce opravdu vracela pravděpodobnosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64): \n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)/1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size)/1000\n",
    "\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / sum(np.exp(x))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # zapamatovani si hodnoty predchoziho skryteho stavu\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        # postupny update skrytého stavu\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "\n",
    "        # vypocet vystupniho vektoru\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teď už můžeme inicializovat síť a zkusit si ji pustit na nějakém vstupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999716]\n",
      " [0.50000284]]\n"
     ]
    }
   ],
   "source": [
    "input = create_one_hot_input('i am very good')\n",
    "rnn = RNN(vocab_size, 2)\n",
    "y, h = rnn.forward(input)\n",
    "prediction = rnn.softmax(y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že síť nám sice nějak funguje, ale není moc užitečná. Problém je, že nijak netrénujeme váhy. K tomu je potřeba si definovat ztrátovou funkci, použijeme cross-entropy loss, která se spočítá jako mínus logaritmus počtu správně predikovaných tříd. Zároveň je potřeba dopsat zpětnou propagaci chyby, aby se síť mohla učit ze svých chyb a updatovat si váhy a skryté stavy. To je v podstatě jen derivace $tanh$, dosazení do vzorečků a použití řetízkového pravidla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64): \n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)/1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size)/1000\n",
    "\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / sum(np.exp(x))  \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # zapamatovani si hodnoty predchoziho skryteho stavu\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # postupny update skrytého stavu\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # vypocet vystupniho vektoru\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    def _backpropagation(self, probs, target, learn_rate=2e-2):\n",
    "        # spocitame derivaci na vystupu podle chyby dL/dy\n",
    "        d_y = probs\n",
    "        d_y[target] -= 1\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # spocitame derivaci na vystupu z chyby pro vahy dL/dWhy a bias dL/dby \n",
    "        d_Why = np.dot(d_y, self.last_hs[n].T)\n",
    "        d_by = d_y\n",
    "\n",
    "        # inicializujeme si nulove dL/dWhh, dL/dWxh, a dL/dbh, budeme si do nich pocitat chybu \n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # spocteme dL/dh for pro posledni vystup h\n",
    "        d_h = np.dot(self.Why.T, d_y)\n",
    "\n",
    "        # backpropagujeme chybu zpet v case pomoci dosazovani do rovnic\n",
    "        for t in reversed(range(n)):\n",
    "            # pomocna hodnota: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += np.dot(temp, self.last_hs[t].T)\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += np.dot(temp, self.last_inputs[t].T)\n",
    "\n",
    "            # dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = np.dot(self.Whh, temp)\n",
    "\n",
    "        # abychom zabranili pirlis velkym gradientum, omezime hodnoty do intervalu [-1,1]\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "                  np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # update vah a biasu pomoci gradient descent\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "        \n",
    "    def fit(self, train_data, train_labels, train):\n",
    "        loss = 0\n",
    "        correct_answers = 0\n",
    "\n",
    "        for x, y in zip(train_data, train_labels):\n",
    "            # dopredny pruchod\n",
    "            out, _ = rnn.forward(x)\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            # spocitani loss/accuracy\n",
    "            loss -= np.log(probs[y])\n",
    "            correct_answers += int(np.argmax(probs) == y)\n",
    "\n",
    "            # zpetny pruchod\n",
    "            if train:\n",
    "                self._backpropagation(probs, y)\n",
    "        return loss/len(train_data), correct_answers/len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní už by naší RNN nemelo nic chybět a měli bychom být schopni ji pustit, natrénovat a dostat nějaké zajímavější výsledky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0\n",
      "Train:\tLoss 0.696 | Accuracy: 0.552\n",
      "Test:\tLoss 0.694 | Accuracy: 0.500\n",
      "--- Epoch 100\n",
      "Train:\tLoss 0.689 | Accuracy: 0.552\n",
      "Test:\tLoss 0.694 | Accuracy: 0.500\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.669 | Accuracy: 0.638\n",
      "Test:\tLoss 0.728 | Accuracy: 0.450\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.173 | Accuracy: 0.931\n",
      "Test:\tLoss 0.068 | Accuracy: 1.000\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.009 | Accuracy: 1.000\n",
      "Test:\tLoss 0.010 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.005 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.005 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.003 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.001 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.001 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = parse_data(train_data)\n",
    "test_features, test_labels = parse_data(test_data)\n",
    "\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "for epoch in range(1001):\n",
    "    train_loss, train_accuracy = rnn.fit(train_features, train_labels, True)\n",
    "    if epoch % 100 == 0:\n",
    "        print('--- Epoch %d' % (epoch))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_accuracy))\n",
    "        \n",
    "        test_loss, test_accuracy = rnn.fit(test_features, test_labels, False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme jak nám úspěšnost natrénovaného modelu postupně stoupá. Můžeme si schválně zkusit pustit natrénovaný model na libovolné větě."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0015597]\n",
      " [0.9984403]]\n"
     ]
    }
   ],
   "source": [
    "inputs = create_one_hot_input('i am not sad')\n",
    "y, h = rnn.forward(inputs)\n",
    "probs = rnn.softmax(y)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sekvenční klasifikace pomocí LSTM\n",
    "\n",
    "Nyní, když chápeme, jak taková základní RNN funguje, se zkusíme podívat na složitější druh RNN -- LSTM sítě. Tyto sítě mají navíc uvnitř sebe paměťovou buňku a mechanizmus, který řídí, jakou informaci si buňka pamatuje a jakou zapomíná, protože pamatovat si celou historii by bylo příliš paměťově náročné. Zkusíme se na ni lépe podívat v následujícím příkladu sekvenční klasifikace. \n",
    "\n",
    "Sekvenční klasifikace je prediktivní modelovací problém, kdy máme na vstupu nějakou sekvenci v prostoru nebo čase a cílem je předpovědět kategorii této sekvence. Složitost tohoto problému spočívá v tom, že jednotlivé sekvence mohou mít různou délku nebo mohou být složeny z rozsáhlého slovníku vstupních hodnot a mohou vyžadovat, aby se model naučil nějaké dlouhodobé závislosti nebo kontext mezi vstupními sekvencemi.\n",
    "\n",
    "Zkusíme se tedy podívat na příklad sekvenční klasifikace pomocí LSTM na IMDB datasetu, což je dataset, který obsahuje slovní popis recenzí 50K filmů a následně klasifikaci, jestli byla recenze pozitivní nebo negativní v poměru 1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problém je, že slovní popis je nějak potřeba převést na číselnou reprezentaci. Naštěstí funkce ```imdb.load_data``` umí načíst data tak, že rovnou slova nahradí čísly a rozdělí je na train a test množiny v poměru 1:1. Navíc data načteme tak, že necháme jen prvních ```top_words``` nejčastějších slova  zbytek nahradíme 0. Dále je potřeba zkrátit nebo doplnit vstupní sekvence pro modelování tak, aby byly všechny stejně dlouhé, délku nastavíme na ```max_len```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "max_length = 500\n",
    "x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní máme připravená data a můžeme si definovat a natrénovat model.\n",
    " - První vrstva je ```Embedding```, která používá vektory délky 32 pro každé slovo. \n",
    " - Další vrstva je ```LSTM``` vrstva, která obsahuje 100 paměťových jednotek (neuronů). \n",
    " - Na závěr použijeme ```Dense``` výstupní vrstvu s jedním neuronem a aktivační funkcí sigmoid k vytvoření predikcí 0 nebo 1, protože se jedná o klasifikační úlohu.\n",
    "\n",
    "Problém modelu je, že se velice snadno overfittuje na na daná trénovací data. Proto se se používají ještě vrstvy ```Dropout```, které spočívají v tom, že během trénování se náhodně některé neurony nastaví na nulu a tím se zamaskují a pro průchod sítí se tedy nepoužijí. Tím se simuluje velký počet sítí s odlišnou strukturou a uzly jsou pak robustnější a omezuje se tím pádem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 280s 710ms/step - loss: 0.5110 - accuracy: 0.7368\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 287s 733ms/step - loss: 0.3003 - accuracy: 0.8791\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 307s 785ms/step - loss: 0.2724 - accuracy: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a4e72ad70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na závěr zkusíme predikovat výstupy na testovacích datech a podívat se, jak je model dobrý. Můžete si třeba zkusit pustit trénování modelu s droupoutem a bez něj a podívat se, jak se budou lišit výsledné accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.30%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generování textu znak po znaku \n",
    "\n",
    "Nyní se podíváme na jiný druh problému -- budeme generovat text znak po znaku, neboli natrénujeme jazykový model tak, že když mu pak dáme sekvenci znaků, tak nám model bude schopný předpovědět další znak. Jako trénovací množinu použijeme texty Nietzscheho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Example script to generate text from Nietzsche's writings.\n",
    "    At least 20 epochs are required before the generated text\n",
    "    starts sounding coherent.\n",
    "    It is recommended to run this script on GPU, as recurrent\n",
    "    networks are quite computationally intensive.\n",
    "    If you try this script on new data, make sure your corpus\n",
    "    has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "# nacteme si vstupni data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = set(text)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# rozdelime text na castecne zavisle sekvence znaku delky maxlen\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# prevedeme text na ciselne vektory\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# vytvorime si model    \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# pomocna funkce k ziskani indexu z pravdepodobnostniho pole\n",
    "def sample(a, temperature=1.0):  \n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    a = a/np.sum(a)\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "# natrenujeme model a po kazde iterace generujeme vystup\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for _ in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skript si samozřejmě můžete pustit, ale trénování poběží neskutečně dlouho. Proto jsme skript pustili na Google Colab a na výsledky se můžete podívat [zde](https://colab.research.google.com/drive/1B7zys275xmpPqahPwNvuYMPLmgvlV3l5) nebo v souboru [*results.txt*](/notebooks/07_rbf_rnn/results.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
